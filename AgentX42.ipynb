{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Cell 1 : paths  +  quick sanity\n",
    "# ===============================\n",
    "import os, glob, json, random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# -------- 1. Base paths --------\n",
    "BASE = Path('/kaggle/input/identity-employees-in-surveillance-cctv')\n",
    "\n",
    "# detect whether train lives under â€œdataset/trainâ€ or â€œdataset/dataset/trainâ€\n",
    "possible_roots = [BASE / 'dataset', BASE / 'dataset' / 'dataset']\n",
    "for candidate in possible_roots:\n",
    "    if (candidate / 'train' / 'images').exists():\n",
    "        ROOT = candidate\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not locate train/images under {possible_roots}\")\n",
    "\n",
    "# define train / unseen-test paths\n",
    "TRAIN_IMG_DIR = ROOT / 'train' / 'images'\n",
    "LABELS_CSV    = ROOT / 'train' / 'labels.csv'\n",
    "UNSEEN_ROOT   = BASE / 'dataset_unseen'\n",
    "TEST_IMG_DIR  = UNSEEN_ROOT / 'unseen_test' / 'images'\n",
    "\n",
    "# -------- 2. Sanity checks --------\n",
    "for p in (TRAIN_IMG_DIR, TEST_IMG_DIR, LABELS_CSV):\n",
    "    assert p.exists(), f\"Missing: {p}\"\n",
    "\n",
    "print(\"Train images :\", len(list(TRAIN_IMG_DIR.glob('*.jpg'))))\n",
    "print(\" Test images :\", len(list(TEST_IMG_DIR.glob('*.jpg'))))\n",
    "\n",
    "# -------- 3. Read & CLEAN labels_df --------\n",
    "labels_df = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "# âœ¨ NEW: drop rows pointing to images that are gone (e.g. face_1147.jpg)\n",
    "mask_exists = labels_df['filename'].apply(lambda f: (TRAIN_IMG_DIR / f).exists())\n",
    "labels_df   = labels_df[mask_exists].reset_index(drop=True)\n",
    "\n",
    "print(f\"After dropping missing images â†’ {len(labels_df):,} rows remain\")\n",
    "display(labels_df.head())\n",
    "\n",
    "# -------- 4. Quick counts (optional) --------\n",
    "emp_counts = Counter(labels_df['emp_id'])\n",
    "print(f\"Unique employees: {len(emp_counts)}\")\n",
    "print(\"Five smallest classes:\", emp_counts.most_common()[-5:])\n",
    "\n",
    "# -------- 5. GPU check (optional) --------\n",
    "print(\"CUDA available :\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name       :\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Cell A : extract 500 JPG frames / employee from reference video\n",
    "#          + save them in EXTRA_DIR  +  create new_rows list\n",
    "# ================================================================\n",
    "import cv2, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "EXTRA_DIR = Path('/kaggle/working/tmp_ref_frames')\n",
    "EXTRA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "new_rows = []                 # â† will be concatenated with labels_df in Cell 2\n",
    "SAMPLES  = 500                # number of evenly-spaced frames per video\n",
    "\n",
    "for emp_dir in (ROOT / 'reference_faces').iterdir():        # emp001, emp002 â€¦\n",
    "    vid_files = list(emp_dir.glob('*.mp4'))\n",
    "    if not vid_files:\n",
    "        continue                               # some employees may have only JPGs\n",
    "    vid = vid_files[0]\n",
    "    cap = cv2.VideoCapture(str(vid))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"âš ï¸ could not open {vid}\")\n",
    "        continue\n",
    "\n",
    "    tot = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # ----- sample SAMPLES indices evenly across the video ----------\n",
    "    for f in np.linspace(0, tot - 1, SAMPLES, dtype=int):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, f)\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            continue\n",
    "        fname = f'{emp_dir.name}_{f:04d}.jpg'               # emp001_0003.jpg â€¦\n",
    "        cv2.imwrite(str(EXTRA_DIR / fname), frame)\n",
    "        new_rows.append({'filename': fname, 'emp_id': emp_dir.name})\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "print(f\"âœ… Saved {len(new_rows):,} extra frames to {EXTRA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Cell 2 : dataset, transforms, stratified split (CCTV hold-out)\n",
    "# ================================================================\n",
    "import subprocess, sys, importlib.util, numpy as np, torch, random\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# 0ï¸âƒ£  Install timm (once per kernel) --------------------------------\n",
    "if importlib.util.find_spec(\"timm\") is None:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"install\",\n",
    "                    \"timm\", \"torchmetrics\", \"--no-progress\"])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1ï¸âƒ£  Build master labels_df\n",
    "#     (Cell 1 already loaded + cleaned labels_df, so we start there)\n",
    "# -------------------------------------------------------------------\n",
    "labels_df = labels_df.copy()                 # make local copy of clean df\n",
    "\n",
    "# 1.0  Append the 500-frame video samples made in Cell A\n",
    "extra_df  = pd.DataFrame(new_rows)           # list produced in Cell A\n",
    "labels_df = pd.concat([labels_df, extra_df], ignore_index=True)\n",
    "\n",
    "# 1.1  Append static reference JPGs as â€œphotoâ€ rows\n",
    "static_rows = []\n",
    "for emp_dir in (ROOT / 'reference_faces').iterdir():\n",
    "    for jpg in emp_dir.glob('*.jpg'):\n",
    "        static_rows.append({'filename': jpg.name,\n",
    "                            'emp_id':   emp_dir.name})\n",
    "        # symlink each static JPG into EXTRA_DIR so IMG_ROOTS can find it\n",
    "        link = EXTRA_DIR / jpg.name\n",
    "        if not link.exists():\n",
    "            link.symlink_to(jpg)\n",
    "labels_df = pd.concat([labels_df, pd.DataFrame(static_rows)],\n",
    "                      ignore_index=True)\n",
    "\n",
    "# 1.2  Final safety filter â€” keep only rows whose file exists\n",
    "IMG_ROOTS = [TRAIN_IMG_DIR, EXTRA_DIR]       # search order for images\n",
    "labels_df = labels_df[\n",
    "    labels_df['filename'].apply(\n",
    "        lambda f: any((root / f).exists() for root in IMG_ROOTS)\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "print(f\"After drop-missing âœ {len(labels_df):,} rows\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2ï¸âƒ£  Encode employee IDs â†’ int labels\n",
    "# -------------------------------------------------------------------\n",
    "employee_ids = sorted(labels_df.emp_id.unique())\n",
    "emp2idx = {e: i for i, e in enumerate(employee_ids)}\n",
    "labels_df[\"label_idx\"] = labels_df.emp_id.map(emp2idx)\n",
    "\n",
    "# 3ï¸âƒ£  Tag each row as CCTV vs photo\n",
    "labels_df[\"source\"] = labels_df.filename.apply(\n",
    "    lambda fn: \"photo\" if (EXTRA_DIR / fn).exists() else \"cctv\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4ï¸âƒ£  Transforms\n",
    "# -------------------------------------------------------------------\n",
    "class JpegCompression:                       # PIL-only artefact\n",
    "    def __init__(self, quality=(30, 60)):\n",
    "        self.qmin, self.qmax = quality\n",
    "    def __call__(self, img: Image.Image):\n",
    "        buf = BytesIO()\n",
    "        q   = random.randint(self.qmin, self.qmax)\n",
    "        img.save(buf, format=\"JPEG\", quality=q)\n",
    "        buf.seek(0)\n",
    "        return Image.open(buf)\n",
    "\n",
    "train_tfms = T.Compose([\n",
    "    T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "    T.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    T.RandomRotation(15),\n",
    "    T.RandomPerspective(0.3, p=0.2),\n",
    "    T.RandomApply([T.GaussianBlur(3)], p=0.2),\n",
    "    T.RandomApply([JpegCompression((30, 60))], p=0.3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406],\n",
    "                [0.229, 0.224, 0.225]),\n",
    "    T.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "val_tfms = T.Compose([\n",
    "    T.Resize(256), T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406],\n",
    "                [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5ï¸âƒ£  Dataset (multi-root lookup)\n",
    "# -------------------------------------------------------------------\n",
    "class FaceDS(Dataset):\n",
    "    def __init__(self, df, tfm):\n",
    "        self.df   = df.reset_index(drop=True)\n",
    "        self.tfm  = tfm\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        for root in IMG_ROOTS:\n",
    "            p = root / row.filename\n",
    "            if p.exists():\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Missing {row.filename}\")\n",
    "        return self.tfm(img), row.label_idx\n",
    "\n",
    "full_ds = FaceDS(labels_df, train_tfms)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6ï¸âƒ£  Stratified 10 % hold-out on CCTV (keep all photos in train)\n",
    "# -------------------------------------------------------------------\n",
    "cctv_idx  = labels_df.index[labels_df.source == \"cctv\"].to_numpy()\n",
    "cctv_lbls = labels_df.label_idx.loc[cctv_idx].to_numpy()\n",
    "\n",
    "# keep singleton classes entirely in train\n",
    "from collections import Counter\n",
    "cnt = Counter(cctv_lbls)\n",
    "singleton_cls = [cls for cls, n in cnt.items() if n == 1]\n",
    "mask_single   = np.isin(cctv_lbls, singleton_cls)\n",
    "single_idx    = cctv_idx[mask_single]\n",
    "multi_idx     = cctv_idx[~mask_single]\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=42)\n",
    "train_multi, val_multi = next(sss.split(multi_idx,\n",
    "                                        labels_df.label_idx.loc[multi_idx]))\n",
    "train_cctv = np.concatenate([single_idx, multi_idx[train_multi]])\n",
    "val_cctv   = multi_idx[val_multi]\n",
    "\n",
    "photo_idx  = labels_df.index[labels_df.source == \"photo\"].to_numpy()\n",
    "\n",
    "train_idx = np.concatenate([train_cctv, photo_idx])\n",
    "val_idx   = val_cctv\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7ï¸âƒ£  DataLoaders\n",
    "# -------------------------------------------------------------------\n",
    "train_ds = Subset(full_ds, train_idx)\n",
    "val_ds   = Subset(FaceDS(labels_df.loc[val_idx], val_tfms),\n",
    "                  np.arange(len(val_idx)))\n",
    "\n",
    "BATCH = 32\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                      num_workers=4, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,\n",
    "                      num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train images: {len(train_ds):,} | Val images: {len(val_ds):,}\")\n",
    "print(\"Smallest training classes:\",\n",
    "      Counter(labels_df.label_idx.loc[train_idx]).most_common()[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick visual sanity-check\n",
    "import matplotlib.pyplot as plt\n",
    "sample_img, _ = full_ds[0]              # any sample after transforms\n",
    "plt.imshow(sample_img.permute(1,2,0).numpy()*0.229+0.485)\n",
    "plt.axis('off'); plt.title('Augmentation example');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Cell 3 : EfficientNet-B0 + ArcFace fine-tune (50 epochs)\n",
    "# ================================================================\n",
    "import timm, torch, math, numpy as np\n",
    "from torch import nn\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "device       = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLASSES  = len(employee_ids)                 # set in Cell 2\n",
    "EPOCHS       = 50\n",
    "CKPT         = '/kaggle/working/effb0_arcface.pt'\n",
    "\n",
    "# â”€â”€ 1ï¸âƒ£  Backbone + 512-D projection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "backbone = timm.create_model(\n",
    "    'efficientnet_b0', pretrained=True, num_classes=0, global_pool='avg'\n",
    ").to(device)\n",
    "proj = nn.Linear(backbone.num_features, 512, bias=False).to(device)\n",
    "\n",
    "# â”€â”€ 2ï¸âƒ£  ArcFace additive-margin head â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, in_f, out_f, s=30.0, m=0.50):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(out_f, in_f))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        self.s, self.m = s, m\n",
    "        self.cos_m, self.sin_m = math.cos(m), math.sin(m)\n",
    "        self.th, self.mm = math.cos(math.pi - m), math.sin(math.pi - m) * m\n",
    "    def forward(self, x, labels):\n",
    "        x_n = nn.functional.normalize(x)\n",
    "        w_n = nn.functional.normalize(self.W)\n",
    "        cos = nn.functional.linear(x_n, w_n)            # cosine Î¸\n",
    "        sin = torch.sqrt(1.0 - torch.clamp(cos**2, 0, 1))\n",
    "        phi = cos * self.cos_m - sin * self.sin_m        # cos(Î¸+m)\n",
    "        phi = torch.where(cos > self.th, phi, cos - self.mm)\n",
    "        one_hot = torch.zeros_like(cos, device=x.device)\n",
    "        one_hot.scatter_(1, labels.view(-1,1), 1.0)\n",
    "        logits = (one_hot * phi + (1.0 - one_hot) * cos) * self.s\n",
    "        return logits\n",
    "\n",
    "arc = ArcFace(512, NUM_CLASSES).to(device)\n",
    "\n",
    "# â”€â”€ 3ï¸âƒ£  Optimizer, warm-up, cosine LR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "params = list(backbone.parameters()) + list(proj.parameters()) + list(arc.parameters())\n",
    "opt    = torch.optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\n",
    "warm   = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=2)\n",
    "sched  = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "metric    = MulticlassAccuracy(num_classes=NUM_CLASSES, average='macro').to(device)\n",
    "best_acc  = 0.0\n",
    "\n",
    "# â”€â”€ 4ï¸âƒ£  Training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    # ---- train ----\n",
    "    backbone.train(); proj.train(); arc.train()\n",
    "    for x, y in train_dl:                       # train_dl from Cell 2\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        feats  = proj(backbone(x))\n",
    "        logits = arc(feats, y)\n",
    "        loss   = criterion(logits, y)\n",
    "\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "    # ---- scheduler ----\n",
    "    (warm if ep <= 2 else sched).step()\n",
    "\n",
    "    # ---- validation ----\n",
    "    backbone.eval(); proj.eval(); metric.reset()\n",
    "    with torch.no_grad():\n",
    "        # pre-normalize weight matrix once per epoch\n",
    "        Wn = nn.functional.normalize(arc.W, dim=1)      # [C,512]\n",
    "        for x, y in val_dl:                             # val_dl from Cell 2\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            feats = proj(backbone(x))\n",
    "            fnorm = nn.functional.normalize(feats, dim=1)\n",
    "            preds = torch.matmul(fnorm, Wn.T).argmax(1) # cosine scores\n",
    "            metric.update(preds, y)\n",
    "\n",
    "    acc = metric.compute().item()\n",
    "    print(f\"Epoch {ep:03d} | Val Macro-Acc {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save({'backbone': backbone.state_dict(),\n",
    "                    'proj'    : proj.state_dict(),\n",
    "                    'arc'     : arc.state_dict()}, CKPT)\n",
    "        print(\"  ğŸ”¥ new best saved\")\n",
    "\n",
    "print(f\"\\nâ–¶ Best validation Macro-Accuracy: {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# after reading labels.csv  (Cell 1 or wherever)\n",
    "# --------------------------------------------------\n",
    "TRAIN_IMG_DIR = Path('/kaggle/input/identity-employees-in-surveillance-cctv'\n",
    "                     '/dataset/dataset/train/images')\n",
    "\n",
    "# keep only rows whose image file is really on disk\n",
    "labels_df = labels_df[\n",
    "    labels_df['filename'].apply(lambda f: (TRAIN_IMG_DIR / f).exists())\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtered labels_df â†’ {len(labels_df):,} rows remain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Cell 4 : build FIQA-filtered gallery + sweep cosine Ï„\n",
    "# ================================================================\n",
    "import sys, os, gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms as T\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# â”€â”€ device & base paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BASE   = Path('/kaggle/input/identity-employees-in-surveillance-cctv')\n",
    "\n",
    "# â”€â”€ updated dataset paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TRAIN_ROOT   = BASE / 'dataset' / 'dataset'\n",
    "REF_DIR      = TRAIN_ROOT / 'reference_faces'\n",
    "EXTRA_DIR = Path('/kaggle/working/tmp_ref_frames')  # updated to where frames were saved\n",
    "\n",
    "# â”€â”€ sanity-check reference_faces â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if not REF_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing required directory: {REF_DIR}\")\n",
    "\n",
    "# â”€â”€ check for extracted frames; skip if none â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if EXTRA_DIR.exists() and any(EXTRA_DIR.glob('*.jpg')):\n",
    "    extra_imgs = list(EXTRA_DIR.glob('*.jpg'))\n",
    "    print(f\"Found {len(extra_imgs)} extracted frames in {EXTRA_DIR}\")\n",
    "    use_extra = True\n",
    "else:\n",
    "    print(f\"âš ï¸  No extracted frames in {EXTRA_DIR}, skipping video-frames step.\")\n",
    "    extra_imgs = []\n",
    "    use_extra = False\n",
    "\n",
    "# â”€â”€ reload trained backbone + projection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CKPT     = '/kaggle/working/effb0_arcface.pt'\n",
    "backbone = timm.create_model('efficientnet_b0', pretrained=False,\n",
    "                             num_classes=0, global_pool='avg').to(device)\n",
    "proj     = nn.Linear(backbone.num_features, 512, bias=False).to(device)\n",
    "\n",
    "ck = torch.load(CKPT, map_location=device)\n",
    "backbone.load_state_dict(ck['backbone'])\n",
    "proj.load_state_dict(ck['proj'])\n",
    "backbone.eval(); proj.eval()\n",
    "\n",
    "# â”€â”€ FIQA quality model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "qa = FaceAnalysis(name='buffalo_s')\n",
    "qa.prepare(ctx_id=0)\n",
    "\n",
    "# â”€â”€ transforms for gallery & val â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_tfms = T.Compose([\n",
    "    T.Resize(256), T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# â”€â”€ build raw gallery (static JPEGs + optional video frames) â”€â”€â”€â”€\n",
    "ref_paths, ref_labels, ref_embs = [], [], []\n",
    "\n",
    "# 1) high-quality reference photos\n",
    "for emp_dir in REF_DIR.iterdir():\n",
    "    for jpg in emp_dir.glob('*.jpg'):\n",
    "        img = Image.open(jpg).convert('RGB')\n",
    "        ref_paths.append(jpg)\n",
    "        ref_labels.append(emp_dir.name)\n",
    "        with torch.no_grad():\n",
    "            emb = proj(backbone(val_tfms(img).unsqueeze(0).to(device))).squeeze().cpu()\n",
    "        ref_embs.append(emb)\n",
    "\n",
    "# 2) optionally include extracted video frames\n",
    "if use_extra:\n",
    "    for jpg in extra_imgs:\n",
    "        img = Image.open(jpg).convert('RGB')\n",
    "        ref_paths.append(jpg)\n",
    "        ref_labels.append(jpg.stem.split('_')[0])\n",
    "        with torch.no_grad():\n",
    "            emb = proj(backbone(val_tfms(img).unsqueeze(0).to(device))).squeeze().cpu()\n",
    "        ref_embs.append(emb)\n",
    "\n",
    "print(\"Raw gallery size :\", len(ref_embs))\n",
    "\n",
    "# â”€â”€ FIQA filtering: drop bottom 30% by det_score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scores = []\n",
    "for p in tqdm(ref_paths, desc=\"FIQA scoring\"):\n",
    "    arr   = np.array(Image.open(p).convert('RGB'))\n",
    "    faces = qa.get(arr)\n",
    "    scores.append(faces[0].det_score if faces else 0.0)\n",
    "\n",
    "threshold = np.percentile(scores, 20)\n",
    "keep_ix   = [i for i, s in enumerate(scores) if s >= threshold]\n",
    "\n",
    "ref_embs   = torch.stack([ref_embs[i] for i in keep_ix])\n",
    "ref_labels = [ref_labels[i]     for i in keep_ix]\n",
    "ref_norm   = nn.functional.normalize(ref_embs, dim=1)\n",
    "print(\"Filtered gallery:\", ref_norm.shape)\n",
    "\n",
    "# â”€â”€ embed validation CCTV set & sweep Ï„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_embs, val_true = [], []\n",
    "idx2emp = {v: k for k, v in emp2idx.items()}\n",
    "\n",
    "for xb, yb in tqdm(val_dl, desc=\"val embeddings\"):\n",
    "    xb = xb.to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = backbone(xb)\n",
    "        embs  = proj(feats).cpu()\n",
    "    val_embs.append(embs)\n",
    "    val_true.extend([idx2emp[int(i)] for i in yb])\n",
    "\n",
    "val_embs = torch.cat(val_embs, dim=0)\n",
    "\n",
    "def macro_acc(true, pred):\n",
    "    classes = sorted(set(true))\n",
    "    return np.mean([\n",
    "        np.mean([t == p for t, p in zip(true, pred) if t == c])\n",
    "        for c in classes\n",
    "    ])\n",
    "\n",
    "best_tau, best_ma = None, -1\n",
    "for Ï„ in np.linspace(0.30, 0.55, 51):\n",
    "    preds = []\n",
    "    for e in val_embs:\n",
    "        sims = ref_norm @ nn.functional.normalize(e, dim=0)\n",
    "        j    = sims.argmax().item()\n",
    "        preds.append(ref_labels[j] if sims[j] >= Ï„ else \"unknown\")\n",
    "    m = macro_acc(val_true, preds)\n",
    "    print(f\"Ï„={Ï„:.2f} â†’ Val Macro-Acc {m:.4f}\")\n",
    "    if m > best_ma:\n",
    "        best_ma, best_tau = m, Ï„\n",
    "\n",
    "print(f\"\\nâ–¶ Best Ï„ = {best_tau:.2f}  (Val Macro-Acc = {best_ma:.4f})\")\n",
    "\n",
    "# â”€â”€ save gallery + threshold for Cell 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "torch.save({\n",
    "    'ref_norm':   ref_norm,\n",
    "    'ref_labels': ref_labels,\n",
    "    'best_tau':   best_tau\n",
    "}, '/kaggle/working/gallery.pt')\n",
    "\n",
    "# cleanup\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Cell 4B : grid-search (P_TH, Ï„) on validation set\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from itertools import product\n",
    "\n",
    "device      = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLASSES = len(employee_ids)\n",
    "\n",
    "# â”€â”€ 1ï¸âƒ£ Redefine ArcFace to match your checkpoint keys â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, in_f, out_f, s=30.0, m=0.50):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(out_f, in_f))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        self.s, self.m = s, m\n",
    "        self.cos_m, self.sin_m = np.cos(m), np.sin(m)\n",
    "        self.th, self.mm = np.cos(np.pi - m), np.sin(np.pi - m) * m\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_n = nn.functional.normalize(x)\n",
    "        w_n = nn.functional.normalize(self.W)\n",
    "        cos = nn.functional.linear(x_n, w_n)\n",
    "        sin = torch.sqrt(1.0 - torch.clamp(cos**2, 0, 1))\n",
    "        phi = torch.where(cos > self.th,\n",
    "                          cos * self.cos_m - sin * self.sin_m,\n",
    "                          cos - self.mm)\n",
    "        one_hot = torch.zeros_like(cos, device=x.device)\n",
    "        one_hot.scatter_(1, y.view(-1,1), 1.0)\n",
    "        logits = (one_hot * phi + (1.0 - one_hot) * cos) * self.s\n",
    "        return logits\n",
    "\n",
    "# â”€â”€ 2ï¸âƒ£ Reload your classifier head to get soft-max predictions â”€â”€â”€\n",
    "ck = torch.load('/kaggle/working/effb0_arcface.pt', map_location=device)\n",
    "\n",
    "backbone = timm.create_model('efficientnet_b0', pretrained=False,\n",
    "                             num_classes=0, global_pool='avg').to(device)\n",
    "proj     = nn.Linear(backbone.num_features, 512, bias=False).to(device)\n",
    "arc      = ArcFace(512, NUM_CLASSES).to(device)\n",
    "\n",
    "backbone.load_state_dict(ck['backbone'])\n",
    "proj.load_state_dict(ck['proj'])\n",
    "arc.load_state_dict( ck['arc'] )\n",
    "\n",
    "backbone.eval(); proj.eval(); arc.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def softmax_vec(x):\n",
    "    feats  = proj(backbone(x))\n",
    "    dummy  = torch.zeros(len(x), dtype=torch.long, device=device)\n",
    "    logits = arc(feats, dummy)\n",
    "    return torch.softmax(logits, dim=1)\n",
    "\n",
    "# â”€â”€ 3ï¸âƒ£ Precompute soft-max on your val set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pvecs = []\n",
    "for imgs, _ in val_dl:\n",
    "    pvecs.append( softmax_vec(imgs.to(device)).cpu() )\n",
    "pvecs_val = torch.cat(pvecs)  # [N_val, NUM_CLASSES]\n",
    "\n",
    "# â”€â”€ 4ï¸âƒ£ Macro-accuracy helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def macro_acc(true, pred):\n",
    "    classes = sorted(set(true))\n",
    "    return np.mean([\n",
    "        np.mean([t==p for t,p in zip(true,pred) if t==c])\n",
    "        for c in classes\n",
    "    ])\n",
    "\n",
    "# (we assume the following variables are in scope from Cell 4):\n",
    "#   ref_norm    # torch.Tensor [N_ref, 512]\n",
    "#   ref_labels  # list[str] of length N_ref\n",
    "#   best_tau    # float from Cell 4\n",
    "#   val_embs    # torch.Tensor [N_val, 512]\n",
    "#   val_true    # list[str] of length N_val\n",
    "\n",
    "# â”€â”€ 5ï¸âƒ£ Grid-search over P_TH âˆˆ [0.50,0.70] and Ï„ Â±0.03 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "P_grid = np.arange(0.50, 0.71, 0.02)\n",
    "T_grid = np.arange(best_tau-0.03, best_tau+0.031, 0.01)\n",
    "\n",
    "best_pair, best_ma = None, -1\n",
    "print(f\"Searching PÃ—Ï„ grid with {len(P_grid)}Ã—{len(T_grid)} combosâ€¦\")\n",
    "\n",
    "for P_TH, Ï„ in product(P_grid, T_grid):\n",
    "    preds = []\n",
    "    for prob_vec, emb in zip(pvecs_val, val_embs):\n",
    "        top_p, top_i = prob_vec.max(0)\n",
    "        if top_p >= P_TH:\n",
    "            preds.append(employee_ids[int(top_i)])\n",
    "        else:\n",
    "            sims = ref_norm @ nn.functional.normalize(emb, dim=0)\n",
    "            j    = int(sims.argmax())\n",
    "            preds.append(ref_labels[j] if sims[j] >= Ï„ else \"unknown\")\n",
    "    m = macro_acc(val_true, preds)\n",
    "    if m > best_ma:\n",
    "        best_ma, best_pair = m, (P_TH, Ï„)\n",
    "    print(f\"P={P_TH:.2f}, Ï„={Ï„:.2f} â†’ Val Macro-Acc {m:.4f}\")\n",
    "\n",
    "print(f\"\\nâ–¶ BEST thresholds: P_TH = {best_pair[0]:.2f}, Ï„ = {best_pair[1]:.2f}   (Val Macro-Acc = {best_ma:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Cell 5 : hybrid soft-max + gallery â†’ submission.csv\n",
    "# ================================================================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import timm\n",
    "from PIL import Image\n",
    "\n",
    "device      = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLASSES = len(employee_ids)\n",
    "CKPT        = '/kaggle/working/effb0_arcface.pt'\n",
    "\n",
    "# â”€â”€ 1ï¸âƒ£ Plug in your best thresholds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "P_TH = best_pair[0]   # from Cell 4B\n",
    "C_TH = best_pair[1]   # from Cell 4B\n",
    "\n",
    "# â”€â”€ 2ï¸âƒ£ Reload backbone + ArcFace head â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# (ArcFace class must already be defined in your notebook)\n",
    "backbone = timm.create_model('efficientnet_b0', pretrained=False,\n",
    "                             num_classes=0, global_pool='avg').to(device)\n",
    "proj     = torch.nn.Linear(backbone.num_features, 512, bias=False).to(device)\n",
    "arc      = ArcFace(512, NUM_CLASSES).to(device)\n",
    "\n",
    "ck = torch.load(CKPT, map_location=device)\n",
    "backbone.load_state_dict( ck['backbone'] )\n",
    "proj.load_state_dict(     ck['proj']     )\n",
    "arc.load_state_dict(      ck['arc']      )\n",
    "\n",
    "backbone.eval(); proj.eval(); arc.eval()\n",
    "\n",
    "# â”€â”€ 3ï¸âƒ£ Define helper functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@torch.no_grad()\n",
    "def softmax_vec(x):\n",
    "    feats  = proj(backbone(x))\n",
    "    dummy  = torch.zeros(len(x), dtype=torch.long, device=device)\n",
    "    logits = arc(feats, dummy)\n",
    "    return torch.softmax(logits, dim=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed(timg):\n",
    "    t = val_tfms(timg).unsqueeze(0).to(device)\n",
    "    return proj(backbone(t)).squeeze().cpu()\n",
    "\n",
    "# â”€â”€ 4ï¸âƒ£ Inference loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rows = []\n",
    "for img_path in sorted(TEST_IMG_DIR.glob('*.jpg')):\n",
    "    pil = Image.open(img_path).convert('RGB')\n",
    "    # classifier branch with softmax\n",
    "    x      = val_tfms(pil).unsqueeze(0).to(device)\n",
    "    pvec   = softmax_vec(x)[0].cpu()\n",
    "    top_p, top_i = pvec.max(0)\n",
    "\n",
    "    # gallery branch with cosine similarity\n",
    "    emb    = embed(pil)\n",
    "    emb_n  = torch.nn.functional.normalize(emb, dim=0)\n",
    "    sims   = ref_norm @ emb_n         # ref_norm & ref_labels from Cell 4\n",
    "    j      = int(sims.argmax())\n",
    "\n",
    "    # hybrid decision\n",
    "    if top_p >= P_TH:\n",
    "        pred = employee_ids[int(top_i)]\n",
    "    elif sims[j] >= C_TH:\n",
    "        pred = ref_labels[j]\n",
    "    else:\n",
    "        pred = \"unknown\"\n",
    "\n",
    "    rows.append((img_path.name, pred))\n",
    "\n",
    "# â”€â”€ 5ï¸âƒ£ Write submission.csv â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sub = pd.DataFrame(rows, columns=['image_name', 'employee_id'])\n",
    "sub.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "print(\"âœ… submission.csv saved:\", sub.shape)\n",
    "display(sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "# torch==2.3.0\n",
    "# torchvision==0.19.0\n",
    "# timm==1.0.6\n",
    "# torchmetrics==1.4.0\n",
    "# insightface==0.7.3\n",
    "# scikit-learn==1.5.0\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12838406,
     "sourceId": 105039,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
